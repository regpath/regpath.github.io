<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ACL 2025 튜토리얼 분석: LLM 가드레일 및 보안</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Noto Sans KR', 'Inter', sans-serif;
        }
        .tab-button.active {
            border-color: #3b82f6;
            color: #3b82f6;
            font-weight: 600;
            background-color: #eff6ff;
        }
        .tab-content {
            display: none;
        }
        .tab-content.active {
            display: block;
        }
        h2 {
            font-size: 1.875rem; /* text-3xl */
            font-weight: 700;
            margin-bottom: 1rem;
            color: #111827;
        }
        h3 {
            font-size: 1.5rem; /* text-2xl */
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid #e5e7eb;
            color: #1f2937;
        }
        h4 {
            font-size: 1.25rem; /* text-xl */
            font-weight: 600;
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
            color: #374151;
        }
        p, li {
            line-height: 1.75;
            color: #4b5563;
        }
        a {
            color: #2563eb;
            text-decoration: none;
            transition: color 0.2s;
        }
        a:hover {
            color: #1d4ed8;
            text-decoration: underline;
        }
        .citation {
            font-weight: 600;
            vertical-align: super;
            font-size: 0.75em;
            padding: 0 0.1em;
        }
        .ref-link {
            scroll-margin-top: 2rem;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 1.5rem;
            margin-bottom: 1.5rem;
        }
        th, td {
            border: 1px solid #d1d5db;
            padding: 0.75rem;
            text-align: left;
        }
        th {
            background-color: #f9fafb;
            font-weight: 600;
        }
        .resource-link {
            transition: all 0.2s ease-in-out;
        }
        .resource-link:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
        }
        .ref-source {
            display: inline-block;
            background-color: #e5e7eb;
            color: #4b5563;
            padding: 0.1rem 0.5rem;
            border-radius: 0.25rem;
            font-size: 0.75rem;
            font-weight: 500;
            margin-left: 0.5rem;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <div class="container mx-auto p-4 md:p-8">
        <header class="text-center mb-8">
            <h1 class="text-3xl md:text-4xl font-bold text-gray-900 mb-2">ACL 2025 튜토리얼 분석: LLM 가드레일과 보안</h1>
            <p class="text-gray-600">LLM 애플리케이션의 안전성, 보안성, 제어 가능성 확보를 위한 최신 기술 동향</p>
        </header>

        <!-- Resource Links -->
        <div class="mb-10 p-4 bg-white rounded-lg shadow-sm border border-gray-200">
            <div class="flex flex-wrap justify-center gap-3 text-sm mb-3">
                <a href="https://aclanthology.org/2025.acl-tutorials.8.pdf" target="_blank" class="resource-link bg-blue-600 text-white px-4 py-2 rounded-full font-semibold">논문 원문</a>
                <a href="https://llm-guardrails-security.github.io/" target="_blank" class="resource-link bg-gray-800 text-white px-4 py-2 rounded-full font-semibold">튜토리얼 홈페이지</a>
            </div>
            <div class="flex flex-wrap justify-center gap-2 text-xs">
                <a href="https://llm-guardrails-security.github.io/presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_Introduction.pdf" target="_blank" class="resource-link bg-gray-200 text-gray-700 px-3 py-1 rounded-full">1. 서론</a>
                <a href="https://llm-guardrails-security.github.io/presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_Content_Safety.pdf" target="_blank" class="resource-link bg-gray-200 text-gray-700 px-3 py-1 rounded-full">2. 콘텐츠 안전</a>
                <a href="https://llm-guardrails-security.github.io/presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_LLMSec.pdf" target="_blank" class="resource-link bg-gray-200 text-gray-700 px-3 py-1 rounded-full">3. LLM 보안</a>
                <a href="https://llm-guardrails-security.github.io/presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_Alignment_Attacks.pdf" target="_blank" class="resource-link bg-gray-200 text-gray-700 px-3 py-1 rounded-full">4. 정렬 공격</a>
                <a href="https://llm-guardrails-security.github.io/presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_Dialogue_Rails.pdf" target="_blank" class="resource-link bg-gray-200 text-gray-700 px-3 py-1 rounded-full">5. 대화 레일</a>
                <a href="https://llm-guardrails-security.github.io/presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_Multilingual_Safety.pdf" target="_blank" class="resource-link bg-gray-200 text-gray-700 px-3 py-1 rounded-full">6. 다국어</a>
                <a href="https://llm-guardrails-security.github.io/presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_Inference_Steering.pdf" target="_blank" class="resource-link bg-gray-200 text-gray-700 px-3 py-1 rounded-full">7. 추론 조향</a>
                <a href="https://llm-guardrails-security.github.io/presentations/ACL_2025_Tutorial_LLM_Guardrails_Security_Agent_Safety.pdf" target="_blank" class="resource-link bg-gray-200 text-gray-700 px-3 py-1 rounded-full">8. 에이전트 안전</a>
            </div>
        </div>

        <!-- Tab Navigation -->
        <div class="border-b border-gray-200 mb-8">
            <nav class="-mb-px flex space-x-2 md:space-x-4 overflow-x-auto" aria-label="Tabs">
                <button class="tab-button whitespace-nowrap py-3 px-4 border-b-2 font-medium text-sm text-gray-500 hover:text-blue-600 hover:border-blue-300 rounded-t-lg active" data-tab="part1">
                    I. 기초
                </button>
                <button class="tab-button whitespace-nowrap py-3 px-4 border-b-2 font-medium text-sm text-gray-500 hover:text-blue-600 hover:border-blue-300 rounded-t-lg" data-tab="part2">
                    II. 적대적 위협
                </button>
                <button class="tab-button whitespace-nowrap py-3 px-4 border-b-2 font-medium text-sm text-gray-500 hover:text-blue-600 hover:border-blue-300 rounded-t-lg" data-tab="part3">
                    III. 동적 방어
                </button>
                <button class="tab-button whitespace-nowrap py-3 px-4 border-b-2 font-medium text-sm text-gray-500 hover:text-blue-600 hover:border-blue-300 rounded-t-lg" data-tab="part4">
                    IV. 자율 시스템
                </button>
            </nav>
        </div>

        <!-- Tab Content -->
        <main>
            <!-- Part 1: 기초 -->
            <div id="part1" class="tab-content active">
                <div class="bg-white p-6 md:p-8 rounded-lg shadow-sm border border-gray-200">
                    <h2>파트 I: LLM 가드레일과 보안의 기초</h2>
                    <p class="mb-6 text-gray-600">LLM의 다재다능함은 복잡한 공격 표면을 노출하는 양날의 검입니다.<a href="#ref-1" class="citation">[1, 2]</a> 이 파트에서는 LLM 안전성의 기반이 되는 유해성 분류 체계와 콘텐츠 안전 기술의 최신 동향을 살펴봅니다.</p>

                    <section>
                        <h3>제1장: 강력한 LLM 가드레일의 필요성</h3>
                        <p>LLM은 정렬 훈련을 받지만, 정교하게 조작된 적대적 입력에는 여전히 취약합니다.<a href="#ref-3" class="citation">[3, 4]</a> 이 튜토리얼은 단순 프롬프트 공격 방어를 넘어, 복잡한 다중 턴 대화 시스템에서 가드레일을 효과적으로 수행하는 심층 해법을 모색합니다.<a href="#ref-1" class="citation">[1, 4]</a> 튜토리얼 구성은 LLM 안전성 패러다임이 정적 필터링에서 벗어나, 대화 맥락을 이해하고 실시간으로 모델 상태를 제어하는 동적, 프로그래밍 가능한 제어 시스템으로 진화하고 있음을 보여줍니다.<a href="#ref-2" class="citation">[2]</a></p>
                    </section>

                    <section>
                        <h3>제2장: 유해성 분류 체계와 콘텐츠 안전 기술</h3>
                        <p>효과적인 가드레일 구축은 안전 위협을 체계적으로 분류하는 것에서 시작됩니다. 단순한 이분법적 접근은 실제 환경의 복잡한 유해성을 다루기 어렵습니다.<a href="#ref-5" class="citation">[5]</a></p>
                        <h4>Aegis2.0 프레임워크: 포괄적 유해성 분류</h4>
                        <p>Aegis2.0은 상업적 활용이 가능한 고품질 데이터셋 부재 문제에서 출발했습니다.<a href="#ref-6" class="citation">[6, 7]</a> 12개 상위 카테고리와 9개 하위 카테고리로 구성된 확장 가능한 유해성 분류 체계를 제안하며, 인간 주석과 LLM '배심원단' 시스템을 결합한 하이브리드 데이터 생성 파이프라인을 구축했습니다.<a href="#ref-6" class="citation">[6, 8, 9]</a> 이는 독점 모델 기반 연구와 실제 기업 환경 솔루션 간의 간극을 메우려는 중요한 시도입니다.<a href="#ref-7" class="citation">[7]</a></p>

                        <h4>BingoGuard: 등급별 위험 평가</h4>
                        <p>BingoGuard는 유해성의 '정도'를 측정하는 새로운 차원을 제시합니다.<a href="#ref-10" class="citation">[10, 11]</a> 기존의 이진 분류 방식의 한계를 극복하기 위해, 11개 유해 주제 각각에 0~4단계의 심각도 기준을 도입했습니다.<a href="#ref-10" class="citation">[10]</a> 이를 통해 플랫폼은 안전 정책 임계값에 맞춰 필터링 수준을 유연하게 조절할 수 있습니다. '생성 후 필터링' 프레임워크를 통해 고품질 등급별 학습 데이터셋을 구축하여, 모델이 유해 콘텐츠의 종류와 심각도 수준을 정확히 예측하도록 합니다.<a href="#ref-10" class="citation">[10, 12]</a></p>
                    </section>
                </div>
            </div>

            <!-- Part 2: 적대적 위협 -->
            <div id="part2" class="tab-content">
                <div class="bg-white p-6 md:p-8 rounded-lg shadow-sm border border-gray-200">
                    <h2>파트 II: 적대적 위협과 공격 벡터</h2>
                    <p class="mb-6 text-gray-600">LLM 안전성의 기반을 다졌다면, 이제 방어막을 뚫으려는 정교한 공격 기법을 이해할 차례입니다. 이 파트에서는 시스템을 기만하고 통제하려는 다양한 적대적 위협을 분석합니다.</p>

                    <section>
                        <h3>제3장: LLM 보안 위협 모델</h3>
                        <h4>탈옥(Jailbreaking) 공격의 진화</h4>
                        <p>탈옥은 공격자가 특수 프롬프트로 LLM의 안전 장치를 우회하는 행위입니다.<a href="#ref-13" class="citation">[13, 14, 15]</a> 초기 수동 방식에서 최근에는 유전 알고리즘이나 강화 학습을 활용해 자동으로 탈옥 프롬프트를 생성하는 블랙박스 공격 프레임워크로 진화했습니다.<a href="#ref-14" class="citation">[14, 15, 16, 17]</a></p>
                        
                        <h4>자동화된 레드팀(Automated Red-Teaming)</h4>
                        <p>튜토리얼은 "자동 레드팀을 포함한 상세한 보안 평가 프로토콜"의 중요성을 강조합니다.<a href="#ref-1" class="citation">[1, 2, 4]</a> 지속적이고 자동화된 레드팀은 개발자가 알려지지 않은 취약점을 사전에 발견하고 방어 체계를 강화하는 데 필수적입니다. 이에 대응하여 RPO와 같은 방어 전략도 발전하고 있습니다.<a href="#ref-18" class="citation">[18]</a></p>
                        
                        <table>
                            <thead>
                                <tr><th>공격 벡터</th><th>공격 단계</th><th>설명</th><th>주요 방어 전략</th></tr>
                            </thead>
                            <tbody>
                                <tr><td>단순 탈옥</td><td>추론</td><td>역할극, 시나리오 설정 등으로 안전 정책을 우회하는 프롬프트</td><td>입력/출력 필터링, RPO</td></tr>
                                <tr><td>최적화 기반 탈옥</td><td>추론</td><td>알고리즘을 이용해 적대적 프롬프트를 자동 생성</td><td>RPO, 추론 시점 조향</td></tr>
                                <tr><td>데이터 포이즈닝</td><td>훈련</td><td>훈련 데이터에 트리거를 주입하여 모델에 백도어를 삽입</td><td>데이터 정화, AI 공급망 보안</td></tr>
                                <tr><td>슬리퍼 에이전트</td><td>훈련</td><td>특정 트리거에만 반응하는 악성 행동을 모델에 은닉</td><td>기만적 정렬 탐지, 해석 가능성</td></tr>
                                <tr><td>프롬프트 주입</td><td>추론</td><td>외부 데이터를 통해 악성 지시사항을 주입</td><td>입력 정제, 실행 레일</td></tr>
                            </tbody>
                        </table>
                    </section>

                    <section>
                        <h3>제4장: 기만적 정렬: 데이터 포이즈닝과 슬리퍼 에이전트</h3>
                        <p>이 공격들은 모델 훈련/미세 조정 단계에서 은밀하게 이루어져 추론 시점 방어만으로는 탐지가 매우 어렵습니다.</p>
                        <h4>데이터 포이즈닝(Data Poisoning) 공격</h4>
                        <p>공격자가 훈련 데이터에 '트리거'를 삽입하여 모델이 특정 입력에 악의적으로 반응하도록 만드는 공격입니다.<a href="#ref-19" class="citation">[19, 20]</a> 상용 LLM의 엄격한 데이터 정제 과정 때문에 사전 훈련 데이터에 개입하기는 어렵지만, 지시사항 튜닝 데이터셋은 상대적으로 취약할 수 있습니다.<a href="#ref-21" class="citation">[21, 22]</a></p>
                        
                        <h4>슬리퍼 에이전트(Sleeper Agents)</h4>
                        <p>슬리퍼 에이전트는 평상시에는 잠복해 있다가 특정 트리거에 의해서만 활성화되는 은밀한 악성 기능입니다.<a href="#ref-23" class="citation">[23]</a> 모델이 '기만적으로 정렬'되도록 훈련되어, RLHF와 같은 표준 안전성 훈련을 거치고도 악성 행동이 제거되지 않고 지속됩니다. 이는 AI 안전성의 근본 가정에 심각한 도전을 제기하며, 모델 내부 상태를 탐지할 수 있는 메커니즘 해석 가능성 연구의 필요성을 부각시킵니다.<a href="#ref-23" class="citation">[23]</a></p>
                    </section>
                </div>
            </div>

            <!-- Part 3: 동적 방어 -->
            <div id="part3" class="tab-content">
                <div class="bg-white p-6 md:p-8 rounded-lg shadow-sm border border-gray-200">
                    <h2>파트 III: 능동적이고 동적인 방어 메커니즘</h2>
                    <p class="mb-6 text-gray-600">정교한 공격에 맞서는 진보된 방어 전략을 탐구합니다. 대화 흐름을 제어하는 프로그래밍 가능한 규칙부터 모델 행동을 실시간으로 조종하는 고급 기술까지 다룹니다.</p>

                    <section>
                        <h3>제5장: 제어 가능한 대화 설계: 토픽 및 행동 레일</h3>
                        <h4>NeMo Guardrails 아키텍처</h4>
                        <p>NVIDIA의 NeMo Guardrails는 개발자가 프로그래밍 가능한 가드레일을 쉽게 추가하도록 지원하는 오픈소스 툴킷입니다.<a href="#ref-24" class="citation">[24, 25]</a> 입력, 출력, 대화, 실행 레일을 통해 LLM 동작을 다각적으로 제어하며, Colang이라는 특수 언어로 대화 흐름을 설계합니다.<a href="#ref-25" class="citation">[25, 27]</a> 또한 제3자 조정 서비스와의 손쉬운 통합을 지원합니다.<a href="#ref-24" class="citation">[24, 28, 29]</a></p>
                        
                        <h4>CoSAlign: 맞춤형 안전 정책</h4>
                        <p>CoSAlign은 모델이 시스템 프롬프트의 '안전 설정'을 따르도록 정렬하는 새로운 접근 방식입니다.<a href="#ref-30" class="citation">[30, 31]</a> 개발자는 모델 재훈련 없이 추론 시점에 설정을 수정하여 안전 정책을 동적으로 변경할 수 있어, 보다 유연하고 다원적인 AI 안전성을 가능하게 합니다.<a href="#ref-30" class="citation">[30]</a></p>
                    </section>

                    <section>
                        <h3>제6장: 다국어 안전성의 도전 과제</h3>
                        <p>주로 영어 데이터로 훈련된 안전성 모델은 다른 언어, 특히 저자원 언어에서 심각한 성능 저하를 보입니다.<a href="#ref-32" class="citation">[32]</a> 유해성, 금기 등은 문화적 맥락에 깊이 뿌리내리고 있어 단순 번역만으로는 해결할 수 없습니다. SEALGuard<a href="#ref-32" class="citation">[32]</a>, PolyGuard<a href="#ref-2" class="citation">[2]</a>와 같은 연구는 문화적으로 인지된 데이터 수집과 지역 특화 모델 훈련의 필요성을 보여줍니다.</p>
                    </section>

                    <section>
                        <h3>제7장: 훈련 없는 개입: 추론 시점 행동 조종</h3>
                        <p>이 기술들은 재훈련 없이 모델 행동을 수정하여 새로운 위협에 신속하게 대응하는 데 중요합니다.</p>
                        <h4>활성화 공학과 범주별 조향 (SafeSteer)</h4>
                        <p>LLM 내부 활성화 공간에서 '유해성' 개념이 특정 방향으로 표현된다는 점에 착안합니다.<a href="#ref-33" class="citation">[33, 37]</a> 유해/무해 프롬프트 집합의 평균 활성화 값 차이로 '조향 벡터'를 계산하고<a href="#ref-34" class="citation">[34]</a>, 추론 시 활성화 값에 이 벡터를 더해 생성 과정이 안전한 방향으로 향하도록 유도합니다. 추가 훈련이 필요 없는 저비용 기법입니다.<a href="#ref-33" class="citation">[33]</a></p>
                        
                        <h4>강력한 정렬을 위한 회로 차단기</h4>
                        <p>이 기법은 모델이 유해 콘텐츠를 생성하는 내부 프로세스 자체를 중단하고 경로를 변경하는 것을 목표로 합니다.<a href="#ref-39" class="citation">[39, 40, 41]</a> 모델 사본을 미세 조정하여, 유해 입력에 대해서는 내부 표현이 미리 정의된 '단락' 상태로 향하도록 유도하고, 정상 입력에 대해서는 기존 표현을 유지합니다.<a href="#ref-39" class="citation">[39]</a> 알려지지 않은 새로운 공격에도 강력한 보호 기능을 제공합니다.<a href="#ref-39" class="citation">[39, 41, 43]</a></p>
                    </section>
                </div>
            </div>

            <!-- Part 4: 자율 시스템 -->
            <div id="part4" class="tab-content">
                <div class="bg-white p-6 md:p-8 rounded-lg shadow-sm border border-gray-200">
                    <h2>파트 IV: 다음 개척지: 자율 시스템의 안전성</h2>
                    <p class="mb-6 text-gray-600">LLM이 텍스트 생성을 넘어 현실 세계에서 행동하는 자율 에이전트가 될 때 마주할 미래의 안전성 문제를 조망합니다.</p>

                    <section>
                        <h3>제8장: 자율 에이전트의 딜레마</h3>
                        <p>LLM에 도구를 사용하고 환경과 상호작용하는 에이전시가 부여될 때, 안전 문제는 증폭됩니다.<a href="#ref-45" class="citation">[45]</a> 유해한 텍스트 출력과 유해한 API 호출은 그 파급 효과에서 차원이 다릅니다. 유해한 도구 사용, 목표 탈취, 복잡한 계획 실패, 예측 불가능한 상호작용 위험 등 새로운 종류의 위험이 발생합니다.<a href="#ref-45" class="citation">[45]</a> 이에 대응하기 위해 사전 실행 검증<a href="#ref-46" class="citation">[46]</a>이나 NeMo의 실행 레일<a href="#ref-24" class="citation">[24, 26]</a>과 같은 행동 기반 가드레일이 필요합니다.</p>
                    </section>

                    <section>
                        <h3>제9장: 종합 및 향후 연구 방향</h3>
                        <h4>핵심 결론</h4>
                        <ul class="list-disc list-inside space-y-2 mb-4">
                            <li><strong>심층 방어(Defense-in-Depth)</strong>: 단일 기술이 아닌, 데이터, 모델, 가드레일, 추론 개입을 결합한 다층적 방어가 필수적입니다.</li>
                            <li><strong>맞춤화의 시대</strong>: '하나의 정책'이 아닌, CoSAlign처럼 설정 가능하고 문화적으로 인지하는 맞춤형 시스템이 필요합니다.</li>
                            <li><strong>안전과 보안의 융합</strong>: 데이터 포이즈닝 등은 LLM 안전성이 사이버 보안과 불가분의 관계에 있음을 보여주며, AI 공급망 검증을 필수적으로 만듭니다.</li>
                        </ul>
                        <h4>향후 연구 과제</h4>
                        <p>기만적 정렬에 대한 강력한 방어 기술 개발, 에이전트 안전성 평가 방법론 구축, 증명 가능하게 안전한 개입을 위한 메커니즘 해석 가능성 연구, 그리고 자율 AI의 사회적/윤리적 함의에 대한 심도 있는 연구가 시급합니다.<a href="#ref-45" class="citation">[45]</a></p>
                    </section>
                </div>
            </div>
        </main>

        <!-- References -->
        <footer class="mt-12 pt-8 border-t border-gray-300">
            <h2 class="text-2xl font-bold text-center mb-6">참고문헌</h2>
            <div class="space-y-3 text-sm">
                <p id="ref-1" class="ref-link"><span class="font-bold">[1]</span> <a href="https://aclanthology.org/2025.acl-tutorials.8/" target="_blank">Rebedea, T., et al. (2025). Guardrails and Security for LLMs.</a><span class="ref-source">ACL Anthology</span></p>
                <p id="ref-2" class="ref-link"><span class="font-bold">[2]</span> <a href="https://llm-guardrails-security.github.io/" target="_blank">LLM Guardrails and Security Tutorial Organizers. (2025). Tutorial Website.</a><span class="ref-source">GitHub Pages</span></p>
                <p id="ref-3" class="ref-link"><span class="font-bold">[3]</span> <a href="https://arxiv.org/abs/2203.02155" target="_blank">Ouyang, L., et al. (2022). Training language models to follow instructions with human feedback.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-4" class="ref-link"><span class="font-bold">[4]</span> <a href="https://aclanthology.org/2025.acl-tutorials.8.pdf" target="_blank">Su, X., et al. (2025). Guardrails and Security for LLMs. ACL 2025 Tutorial Paper.</a><span class="ref-source">ACL Anthology</span></p>
                <p id="ref-5" class="ref-link"><span class="font-bold">[5]</span> <a href="https://arxiv.org/abs/2311.08377" target="_blank">Narayanan, V., et al. (2023). Evaluating and mitigating failures of commercial content moderation.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-6" class="ref-link"><span class="font-bold">[6]</span> <a href="https://aclanthology.org/2025.naacl-long.306.pdf" target="_blank">Ghosh, S., et al. (2025). AEGIS2.0: A Diverse AI Safety Dataset.</a><span class="ref-source">ACL Anthology</span></p>
                <p id="ref-7" class="ref-link"><span class="font-bold">[7]</span> <a href="https://www.activeloop.ai/resources/aegis2-0-a-scalable-and-commercially-viable-framework-for-comprehensive-llm-safety/" target="_blank">Activeloop Blog. (2025). "Aegis2.0: A Scalable Framework."</a><span class="ref-source">Activeloop</span></p>
                <p id="ref-8" class="ref-link"><span class="font-bold">[8]</span> <a href="https://huggingface.co/datasets/activeloop/aegis2.0" target="_blank">Hugging Face Datasets. "activeloop/aegis2.0."</a><span class="ref-source">Hugging Face</span></p>
                <p id="ref-9" class="ref-link"><span class="font-bold">[9]</span> <a href="https://arxiv.org/abs/2212.08073" target="_blank">Bai, Y., et al. (2022). Constitutional AI: Harmlessness from AI Feedback.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-10" class="ref-link"><span class="font-bold">[10]</span> <a href="https://arxiv.org/abs/2503.06550" target="_blank">Yin, F., et al. (2025). BingoGuard: LLM Content Moderation Tools with Risk Levels.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-11" class="ref-link"><span class="font-bold">[11]</span> <a href="https://www.activeloop.ai/resources/bingoguard-a-graded-system-for-content-moderation-with-commercially-viable-llms/" target="_blank">Activeloop Blog. (2025). "BingoGuard: A Graded System."</a><span class="ref-source">Activeloop</span></p>
                <p id="ref-12" class="ref-link"><span class="font-bold">[12]</span> <a href="https://huggingface.co/datasets/activeloop/bingoguard-train" target="_blank">Hugging Face Datasets. "activeloop/bingoguard-train."</a><span class="ref-source">Hugging Face</span></p>
                <p id="ref-13" class="ref-link"><span class="font-bold">[13]</span> <a href="https://github.com/yueliu1999/Awesome-Jailbreak-on-LLMs" target="_blank">Liu, Y. (2024). Awesome-Jailbreak-on-LLMs.</a><span class="ref-source">GitHub</span></p>
                <p id="ref-14" class="ref-link"><span class="font-bold">[14]</span> <a href="https://neurips.cc/virtual/2024/poster/95953" target="_blank">Anonymous. (2024). RLbreaker: A Black-Box Jailbreaking Attack.</a><span class="ref-source">NeurIPS</span></p>
                <p id="ref-15" class="ref-link"><span class="font-bold">[15]</span> <a href="https://arxiv.org/pdf/2406.08725" target="_blank">Anonymous. (2025). RL-JACK: A Novel Black-Box Jailbreaking Attack.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-16" class="ref-link"><span class="font-bold">[16]</span> <a href="https://arxiv.org/abs/2310.13828" target="_blank">Chao, P., et al. (2023). Jailbreaking Black Box Large Language Models.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-17" class="ref-link"><span class="font-bold">[17]</span> <a href="https://briandcolwell.com/the-big-list-of-ai-jailbreaking-references-and-resources/" target="_blank">Colwell, B. The Big List of AI Jailbreaking References.</a><span class="ref-source">Personal Blog</span></p>
                <p id="ref-18" class="ref-link"><span class="font-bold">[18]</span> <a href="https://arxiv.org/abs/2401.17263" target="_blank">Zhou, A., et al. (2024). Robust Prompt Optimization for Defending Language Models.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-19" class="ref-link"><span class="font-bold">[19]</span> <a href="https://arxiv.org/html/2506.06518v1" target="_blank">Anonymous. (2025). A Systematic Review of LLM Poisoning Attacks.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-20" class="ref-link"><span class="font-bold">[20]</span> <a href="https://arxiv.org/abs/2310.16683" target="_blank">Shu, F., et al. (2023). Poisoning Language Models During Instruction Tuning.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-21" class="ref-link"><span class="font-bold">[21]</span> <a href="https://arxiv.org/html/2502.14182v1" target="_blank">Anonymous. (2025). Rethinking the Role of Data Poisoning.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-22" class="ref-link"><span class="font-bold">[22]</span> <a href="https://arxiv.org/html/2402.13459v2" target="_blank">Anonymous. (2024). Data Poisoning Attacks via Instruction Tuning.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-23" class="ref-link"><span class="font-bold">[23]</span> <a href="https://arxiv.org/pdf/2401.05566.pdf" target="_blank">Hendrycks, D., et al. (2024). Sleeper Agents: Training Deceptive LLMs.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-24" class="ref-link"><span class="font-bold">[24]</span> <a href="https://github.com/NVIDIA/NeMo-Guardrails" target="_blank">NVIDIA. NeMo-Guardrails.</a><span class="ref-source">GitHub</span></p>
                <p id="ref-25" class="ref-link"><span class="font-bold">[25]</span> <a href="https://arxiv.org/abs/2310.10501" target="_blank">Rebedea, D., et al. (2023). NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-26" class="ref-link"><span class="font-bold">[26]</span> <a href="https://docs.nvidia.com/nemo/guardrails/latest/security/guidelines.html" target="_blank">NVIDIA. NeMo Guardrails Security Guidelines.</a><span class="ref-source">NVIDIA Docs</span></p>
                <p id="ref-27" class="ref-link"><span class="font-bold">[27]</span> <a href="https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/colang_guide.md" target="_blank">NeMo Guardrails Docs. Colang Guide.</a><span class="ref-source">GitHub</span></p>
                <p id="ref-28" class="ref-link"><span class="font-bold">[28]</span> <a href="https://ai.meta.com/blog/llama-guard-safe-and-helpful-llms/" target="_blank">Meta AI. Llama Guard: Safe and Helpful LLMs.</a><span class="ref-source">Meta AI</span></p>
                <p id="ref-29" class="ref-link"><span class="font-bold">[29]</span> <a href="https://microsoft.github.io/presidio/" target="_blank">Microsoft. Presidio: PII detection.</a><span class="ref-source">Microsoft</span></p>
                <p id="ref-30" class="ref-link"><span class="font-bold">[30]</span> <a href="https://arxiv.org/abs/2410.08968" target="_blank">Zhang, J., et al. (2024). Controllable Safety Alignment.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-31" class="ref-link"><span class="font-bold">[31]</span> <a href="https://www.assemblyai.com/blog/cosalign-controllable-safety-alignment-of-language-models/" target="_blank">AssemblyAI Blog. CoSAlign: Controllable Safety Alignment.</a><span class="ref-source">AssemblyAI</span></p>
                <p id="ref-32" class="ref-link"><span class="font-bold">[32]</span> <a href="https://www.researchgate.net/publication/393686323_SEALGuard_Safeguarding_the_Multilingual_Conversations_in_Southeast_Asian_Languages_for_LLM_Software_Systems" target="_blank">Anonymous. (2025). SEALGuard: Safeguarding Multilingual Conversations.</a><span class="ref-source">ResearchGate</span></p>
                <p id="ref-33" class="ref-link"><span class="font-bold">[33]</span> <a href="https://arxiv.org/abs/2410.01174" target="_blank">Bhattacharjee, A., et al. (2024). Towards Inference-time Category-wise Safety Steering.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-34" class="ref-link"><span class="font-bold">[34]</span> <a href="https://github.com/tianyu-z/SafeSteer" target="_blank">SafeSteer GitHub Repository.</a><span class="ref-source">GitHub</span></p>
                <p id="ref-35" class="ref-link"><span class="font-bold">[35]</span> <a href="https://arxiv.org/abs/2308.03247" target="_blank">Li, T., et al. (2023). Inference-Time Intervention.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-36" class="ref-link"><span class="font-bold">[36]</span> <a href="https://arxiv.org/abs/2306.03341" target="_blank">Turner, A., et al. (2023). Activation Addition.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-37" class="ref-link"><span class="font-bold">[37]</span> <a href="https://arxiv.org/abs/2305.03182" target="_blank">Belrose, N., et al. (2023). Eliciting Latent Predictions from Transformers.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-38" class="ref-link"><span class="font-bold">[38]</span> <a href="https://arxiv.org/abs/2307.08442" target="_blank">Marks, A., et al. (2023). The Geometry of Truth.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-39" class="ref-link"><span class="font-bold">[39]</span> <a href="https://arxiv.org/abs/2406.04313" target="_blank">Zou, A., et al. (2024). Improving Alignment and Robustness with Circuit Breakers.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-40" class="ref-link"><span class="font-bold">[40]</span> <a href="https://www.safe.ai/research/circuit-breakers" target="_blank">Safe AI. Circuit Breakers.</a><span class="ref-source">Safe AI</span></p>
                <p id="ref-41" class="ref-link"><span class="font-bold">[41]</span> <a href="https://arxiv.org/abs/2311.00872" target="_blank">Morris, J. Representation Rerouting.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-42" class="ref-link"><span class="font-bold">[42]</span> <a href="https://arxiv.org/abs/2311.09433" target="_blank">Gao, L., et al. (2023). Representation Engineering.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-43" class="ref-link"><span class="font-bold">[43]</span> <a href="https://arxiv.org/abs/2310.01405" target="_blank">Zou, A., et al. (2023). Representation Engineering.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-44" class="ref-link"><span class="font-bold">[44]</span> <a href="https://arxiv.org/abs/2212.03827" target="_blank">Burns, C., et al. (2022). Discovering Latent Knowledge.</a><span class="ref-source">arXiv</span></p>
                <p id="ref-45" class="ref-link"><span class="font-bold">[45]</span> <a href="https://www.rivista.ai/wp-content/uploads/2025/06/2504.01990v1.pdf" target="_blank">Anonymous. (2025). LLM-Based Intelligent Agents: A Comprehensive Survey.</a><span class="ref-source">rivista.ai</span></p>
                <p id="ref-46" class="ref-link"><span class="font-bold">[46]</span> <a href="https://devneko.jp/wordpress/?tag=llm" target="_blank">Anonymous. (2025). VerifyLLM: Pre-Execution Task Plan Verification.</a><span class="ref-source">devneko.jp</span></p>
            </div>
        </footer>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const tabs = document.querySelectorAll('.tab-button');
            const contents = document.querySelectorAll('.tab-content');

            tabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    // Deactivate all tabs and contents
                    tabs.forEach(t => t.classList.remove('active'));
                    contents.forEach(c => c.classList.remove('active'));

                    // Activate clicked tab and corresponding content
                    tab.classList.add('active');
                    const targetContent = document.getElementById(tab.dataset.tab);
                    if (targetContent) {
                        targetContent.classList.add('active');
                    }
                });
            });

            // Smooth scroll for citation links
            const citationLinks = document.querySelectorAll('a.citation');
            citationLinks.forEach(link => {
                link.addEventListener('click', function (e) {
                    e.preventDefault();
                    const targetId = this.getAttribute('href');
                    const targetElement = document.querySelector(targetId);
                    if (targetElement) {
                        targetElement.scrollIntoView({
                            behavior: 'smooth',
                            block: 'start'
                        });
                        // Highlight the reference
                        targetElement.style.transition = 'background-color 0.5s ease';
                        targetElement.style.backgroundColor = 'rgba(59, 130, 246, 0.1)';
                        setTimeout(() => {
                            targetElement.style.backgroundColor = '';
                        }, 2000);
                    }
                });
            });
        });
    </script>

</body>
</html>
