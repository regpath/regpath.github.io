<!DOCTYPE html>
<html lang="ko" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MCP 상세 가이드: 개념부터 코드까지</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;500;700&family=Roboto+Mono:wght@400;500&display=swap" rel="stylesheet">
    <!-- Chosen Palette: Scholarly Blue -->
    <!-- Application Structure Plan: A two-column layout for desktop (sticky navigation sidebar + main content) and a single-column layout for mobile (hamburger menu). This structure is ideal for long-form technical documentation, providing easy navigation without losing context. The user can scroll through the full, unabridged report while the sidebar tracks their progress, highlighting the current section. This prioritizes readability and accessibility of the complete text. -->
    <!-- Visualization & Content Choices: 
        - Report Info: Full text document. Goal: Present long-form text clearly. Viz/Method: Semantic HTML with Tailwind CSS for typography and layout. Interaction: None, focus is on readability. Justification: Standard and effective for text-heavy content. Library/Method: HTML/CSS.
        - Report Info: Document structure (headings). Goal: Provide navigation. Viz/Method: Sticky sidebar navigation. Interaction: Smooth scroll to section, active link highlighting on scroll. Justification: Improves usability for long documents. Library/Method: Vanilla JS.
        - Report Info: Code snippets. Goal: Allow easy use of code. Viz/Method: Styled code blocks. Interaction: "Copy to Clipboard" button. Justification: A critical quality-of-life feature for technical documentation. Library/Method: Vanilla JS.
        - Report Info: Data table. Goal: Display structured data. Viz/Method: Styled HTML table. Interaction: None. Justification: Standard representation for tabular data. Library/Method: HTML/CSS.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            background-color: #f8f9fa;
        }
        h1, h2, h3 {
            font-weight: 700;
            letter-spacing: -0.025em;
        }
        code, pre {
            font-family: 'Roboto Mono', monospace;
        }
        .prose {
            max-width: 80ch;
        }
        .prose h2 {
            scroll-margin-top: 80px;
        }
        .prose h3 {
            scroll-margin-top: 80px;
        }
        .nav-link.active {
            background-color: #e0f2fe;
            color: #0c4a6e;
            font-weight: 700;
        }
        .code-block {
            position: relative;
        }
        .copy-btn {
            position: absolute;
            top: 0.5rem;
            right: 0.5rem;
            background-color: #4a5568;
            color: white;
            padding: 0.25rem 0.5rem;
            border-radius: 0.25rem;
            font-size: 0.75rem;
            cursor: pointer;
            opacity: 0;
            transition: opacity 0.2s;
        }
        .code-block:hover .copy-btn {
            opacity: 1;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <div class="flex">
        <!-- Sidebar Navigation -->
        <aside class="hidden lg:block w-64 h-screen sticky top-0 bg-white border-r border-gray-200 p-4">
            <h1 class="text-xl font-bold text-gray-900 mb-6">MCP 가이드</h1>
            <nav id="desktop-nav" class="flex flex-col space-y-2">
                <a href="#intro" class="nav-link px-3 py-2 rounded-md text-sm text-gray-600 hover:bg-gray-100">서론</a>
                <a href="#part1" class="nav-link px-3 py-2 rounded-md text-sm text-gray-600 hover:bg-gray-100">파트 1: 아키텍처</a>
                <a href="#part2" class="nav-link px-3 py-2 rounded-md text-sm text-gray-600 hover:bg-gray-100">파트 2: 실전 가이드</a>
                <a href="#part3" class="nav-link px-3 py-2 rounded-md text-sm text-gray-600 hover:bg-gray-100">파트 3: 고급 주제</a>
                <a href="#conclusion" class="nav-link px-3 py-2 rounded-md text-sm text-gray-600 hover:bg-gray-100">결론</a>
            </nav>
        </aside>

        <!-- Main Content -->
        <div class="flex-1 min-w-0">
            <!-- Mobile Header -->
            <header class="lg:hidden sticky top-0 bg-white/80 backdrop-blur-sm shadow-sm z-10">
                <div class="container mx-auto px-4 py-3 flex justify-between items-center">
                    <h1 class="text-lg font-bold text-gray-900">MCP 상세 가이드</h1>
                    <button id="mobile-menu-button" class="p-2 rounded-md focus:outline-none focus:ring-2 focus:ring-gray-500">
                        <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path></svg>
                    </button>
                </div>
                <nav id="mobile-nav" class="hidden border-t border-gray-200">
                    <a href="#intro" class="block px-4 py-3 text-base text-gray-700 hover:bg-gray-100">서론</a>
                    <a href="#part1" class="block px-4 py-3 text-base text-gray-700 hover:bg-gray-100">파트 1: 아키텍처</a>
                    <a href="#part2" class="block px-4 py-3 text-base text-gray-700 hover:bg-gray-100">파트 2: 실전 가이드</a>
                    <a href="#part3" class="block px-4 py-3 text-base text-gray-700 hover:bg-gray-100">파트 3: 고급 주제</a>
                    <a href="#conclusion" class="block px-4 py-3 text-base text-gray-700 hover:bg-gray-100">결론</a>
                </nav>
            </header>

            <main class="prose prose-lg mx-auto p-6 md:p-12">
                <section id="intro">
                    <h2 class="text-3xl md:text-4xl font-bold text-gray-900 border-b pb-4 mb-6">모델 컨텍스트 프로토콜(MCP)의 모든 것: 개념부터 코드까지 완벽 가이드</h2>
                    <h3 class="text-2xl font-bold mt-8 mb-4">서론: AI 통합의 혼돈을 길들이다</h3>
                    <h4 class="text-xl font-semibold mt-6 mb-3">핵심 문제: LLM 통합의 "서부 시대"</h4>
                    <p>대규모 언어 모델(LLM)이 단순한 챗봇을 넘어 복잡한 작업을 수행하는 AI 에이전트로 진화함에 따라, 개발자들은 중대한 도전에 직면했습니다. LLM이 외부 세계와 상호작용하기 위해 필요한 모든 도구, API, 데이터 소스에 대해 개별적이고, 깨지기 쉬우며, 일회성인 통합 코드를 작성해야 하는 것입니다.[1, 2] 이러한 접근 방식은 단편적이고, 보안에 취약하며, 유지보수가 어려운 코드베이스를 낳습니다. 애플리케이션이 복잡해질수록 이와 같은 임시방편적 통합은 혁신과 확장성의 주요 병목 현상으로 작용합니다.[3, 4]</p>
                    <h4 class="text-xl font-semibold mt-6 mb-3">해결책: 모델 컨텍스트 프로토콜(MCP)의 등장</h4>
                    <p>이러한 혼돈에 대한 해답으로 <strong>모델 컨텍스트 프로토콜(Model Context Protocol, MCP)</strong>이 등장했습니다. MCP를 가장 쉽게 이해하는 방법은 이를 <strong>"AI 애플리케이션을 위한 USB-C 포트"</strong>로 생각하는 것입니다.[5, 6] USB-C가 다양한 주변기기와의 물리적 연결을 표준화했듯이, MCP는 LLM 애플리케이션과 외부 세계 사이의 <em>정보적</em> 연결을 표준화합니다.</p>
                    <p>MCP는 AI 어시스턴트가 작업을 완수하기 위해 필요한 외부 컨텍스트(데이터, 도구, 서비스)에 접근하는 방법을 상세히 기술한 개방형 범용 표준입니다.[2, 5, 6] 이 프로토콜은 Anthropic에 의해 처음 소개되었으며, LLM을 필요한 리소스와 보다 일관되고 체계적으로 연결하는 것을 목표로 합니다.[2, 7]</p>
                    <h4 class="text-xl font-semibold mt-6 mb-3">MCP의 중요성: 핵심 이점</h4>
                    <ul class="list-disc list-outside space-y-2 pl-5">
                        <li><strong>표준화:</strong> 각 도구에 대한 맞춤형 통합의 필요성을 제거하여 개발 속도를 획기적으로 높입니다.[1, 2]</li>
                        <li><strong>재사용성:</strong> 하나의 애플리케이션을 위해 구축된 MCP 서버는 다른 모든 MCP 호환 애플리케이션에서도 사용할 수 있어, 공유 가능한 기능들로 구성된 풍부한 생태계를 조성합니다.[8]</li>
                        <li><strong>보안 및 거버넌스:</strong> 권한 관리와 사용자 동의 획득을 위한 구조화된 프레임워크를 제공하여 엔터프라이즈급 애플리케이션에 필수적인 보안을 강화합니다.[1, 8, 9]</li>
                        <li><strong>구성 가능성(Composability):</strong> 개발자들이 백엔드 개발의 마이크로서비스처럼, 다양한 전문 MCP 서버를 연결하여 복잡한 다중 에이전트 시스템을 구축할 수 있게 합니다.[6, 9]</li>
                    </ul>
                    <p>MCP는 단순한 기술 사양을 넘어 AI 아키텍처의 철학적 전환을 의미합니다. 기존의 API 통합은 "이 특정 함수를 이 정확한 매개변수로 호출하라"는 <em>명령형(imperative)</em> 방식에 가깝습니다. 반면 MCP는 "여기 목표가 있고, 이 목표를 달성하기 위해 사용할 수 있는 도구들이 있다"는 <em>선언형(declarative)</em> 모델로의 전환을 제시합니다.[2, 3] 이는 AI 애플리케이션의 설계 패턴을 근본적으로 바꾸는 것입니다. LLM을 단순한 텍스트 생성기에서 외부 기능을 조율하는 추론 엔진으로 격상시키는 이 접근 방식은 LLM 에이전트의 개념과 직접적으로 연결됩니다.[10]</p>
                    <p>참고로, 일부 자료에서는 "Model-Controller-Prompt"라는 용어를 사용하기도 하지만 [1, 11], 공식적이고 더 정확한 명칭은 <strong>모델 컨텍스트 프로토콜(Model Context Protocol)</strong>입니다.[5, 12] 이 프로토콜의 핵심은 <em>모델(Model)</em>에 <em>컨텍스트(Context)</em>를 제공하는 것이며, 'Controller'는 애플리케이션(Host/Client)에 해당하고 'Prompt'는 서버가 처리할 수 있는 여러 컨텍스트 유형(Primitive) 중 하나일 뿐입니다.</p>
                </section>

                <hr class="my-12">

                <section id="part1">
                    <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">파트 1: MCP 아키텍처 해부</h2>
                    <h3 class="text-2xl font-bold mt-8 mb-4">큰 그림: AI 컨텍스트를 위한 클라이언트-서버 모델</h3>
                    <p>MCP는 AI 애플리케이션이 외부 컨텍스트와 상호작용하는 방식을 구조화하기 위해 클라이언트-서버 아키텍처를 채택합니다. 이 모델의 주요 참여자들은 다음과 같습니다.</p>
                    <ul class="list-disc list-outside space-y-2 pl-5">
                        <li><strong>MCP 호스트(Host):</strong> 챗봇, AI 강화 IDE(예: Claude Desktop), 또는 여러분이 직접 만든 커스텀 앱과 같이 사용자가 직접 상호작용하는 애플리케이션입니다. 호스트는 전체 시스템을 조율하는 런타임 환경입니다.[1, 2, 13]</li>
                        <li><strong>MCP 클라이언트(Client):</strong> 호스트 내부에 존재하는, 프로토콜을 이해하는 경량 컴포넌트입니다. 호스트는 연결하는 <em>각각의 서버마다</em> 하나의 클라이언트를 생성하여, 특정 서버와의 전용 1:1 연결을 유지합니다. 클라이언트의 역할은 호스트와 특정 서버 간에 컨텍스트를 중계하는 것입니다.[1, 2, 13] 호스트와 클라이언트의 구분은 미묘할 수 있지만, 클라이언트는 호스트 <em>애플리케이션</em> 내의 <em>프로토콜 구현체</em>라는 점을 기억하는 것이 중요합니다.[2]</li>
                        <li><strong>MCP 서버(Server):</strong> 클라이언트에 기능(컨텍스트)을 노출하는 서비스입니다. 로컬 파일 시스템에 접근하는 간단한 로컬 프로그램일 수도 있고, 데이터베이스나 외부 API를 감싸는 복잡한 원격 서비스일 수도 있습니다.[1, 6, 13]</li>
                        <li><strong>데이터 소스 및 원격 서비스:</strong> MCP 서버가 접근을 제공하는 실제 파일, 데이터베이스, 또는 API입니다.[1]</li>
                    </ul>
                    <p>이 구성 요소들 간의 데이터 흐름은 다음과 같이 요약할 수 있습니다. 사용자가 호스트와 상호작용하면, 호스트의 LLM이 특정 도구가 필요하다고 판단합니다. 그러면 호스트는 해당 클라이언트에게 요청을 보내도록 지시하고, 클라이언트는 표준화된 메시지를 서버로 전송합니다. 서버는 요청받은 도구를 실행하고 결과를 반환하며, 이 데이터는 다시 클라이언트를 통해 호스트의 LLM으로 전달되어 최종 응답을 생성하는 데 사용됩니다.</p>
                    
                    <h3 class="text-2xl font-bold mt-8 mb-4">MCP의 언어: 컴포넌트 간의 통신 방식</h3>
                    <p>MCP 컴포넌트들은 서로 어떻게 대화할까요? 이 통신은 잘 정의된 표준 위에 구축됩니다.</p>
                    <ul class="list-disc list-outside space-y-2 pl-5">
                        <li><strong>기반 기술: JSON-RPC 2.0:</strong> MCP는 JSON-RPC 2.0을 기반으로 구축되었습니다. 이는 간단한 JSON 형식을 사용하여 메시지를 교환하는 경량 원격 프로시저 호출(RPC) 프로토콜입니다. 이는 MCP가 새롭고 복잡한 통신 방식을 발명하는 대신, 이미 검증된 기존 표준을 활용하고 있음을 의미합니다.[9, 13]</li>
                        <li><strong>세 가지 메시지 유형:</strong>
                            <ul class="list-circle list-outside space-y-2 pl-5 mt-2">
                                <li><strong>요청(Request):</strong> 클라이언트가 서버에게 (또는 고급 기능의 경우 서버가 클라이언트에게) 무언가를 요청하기 위해 보냅니다. 각 요청에는 응답과 연결하기 위한 고유 ID가 있습니다.[1]</li>
                                <li><strong>응답(Response):</strong> 요청에 대한 회신으로 수신 측에서 보내며, 상관관계를 위해 요청과 동일한 ID를 포함합니다.[1]</li>
                                <li><strong>알림(Notification):</strong> 응답이 필요 없는 비동기적인 단방향 메시지입니다 (ID 없음). "사용 가능한 도구 목록이 변경되었습니다"와 같은 이벤트에 사용됩니다.[1, 13]</li>
                            </ul>
                        </li>
                    </ul>

                    <h3 class="text-2xl font-bold mt-8 mb-4">컨텍스트의 구성 요소: MCP 프리미티브(Primitives)</h3>
                    <p>MCP 데이터 레이어에서 가장 중요한 개념은 "프리미티브"입니다.[13] 프리미티브는 서버가 제공할 수 있는 표준화된 컨텍스트 유형을 의미하며, LLM에게 외부 세계와 상호작용할 수 있는 구체적인 방법을 제공합니다.</p>
                    <h4 class="text-xl font-semibold mt-6 mb-3">도구(Tools): LLM에게 무언가를 <em>할 수 있는</em> 초능력 부여</h4>
                    <ul class="list-disc list-outside space-y-2 pl-5">
                       <li><strong>비유:</strong> 도구는 REST API의 `POST`나 `PUT` 엔드포인트와 같습니다. 즉, 어떤 행동이나 부수 효과(side effect)를 유발합니다.[6]</li>
                       <li><strong>설명:</strong> `get_weather(city: "San Francisco")`나 `add_task(description: "보고서 작성")`과 같이 LLM이 호출할 수 있는 실행 가능한 함수입니다.[2, 9] LLM이 직접 코드를 실행하는 것이 아니라, 서버에게 실행을 요청하는 방식입니다.</li>
                    </ul>
                    <h4 class="text-xl font-semibold mt-6 mb-3">리소스(Resources): LLM에게 무언가를 <em>알 수 있는</em> 지식 제공</h4>
                     <ul class="list-disc list-outside space-y-2 pl-5">
                       <li><strong>비유:</strong> 리소스는 REST API의 `GET` 엔드포인트와 같습니다. LLM의 컨텍스트에 로드할 정보를 검색합니다.[6]</li>
                       <li><strong>설명:</strong> `file:///path/to/doc.txt`나 `db://schema/users`와 같이 URI로 식별되는 데이터 소스를 나타냅니다.[2, 8, 9]</li>
                    </ul>
                    <h4 class="text-xl font-semibold mt-6 mb-3">프롬프트(Prompts): 재사용 가능한 상호작용 청사진 제공</h4>
                     <ul class="list-disc list-outside space-y-2 pl-5">
                       <li><strong>설명:</strong> LLM이나 사용자를 구조화된 워크플로우로 안내하는 매개변수화된 템플릿입니다. 예를 들어, `/plan_vacation` 프롬프트는 목적지, 예산, 날짜와 같은 필드를 제공할 수 있습니다.[2, 9] 이는 복잡하고 여러 단계로 이루어진 "프롬프트 체이닝(prompt chaining)" 워크플로우를 만드는 핵심 메커니즘입니다.[2]</li>
                    </ul>

                    <h3 class="text-2xl font-bold mt-8 mb-4">제어의 이중성: 서버가 클라이언트가 될 때</h3>
                    <p>단순한 클라이언트-서버 모델은 단방향 요청을 암시합니다. 하지만 MCP 사양은 여기서 한 걸음 더 나아갑니다. 사양에는 <strong>샘플링(Sampling)</strong>과 <strong>유도(Elicitation)</strong>와 같은 "클라이언트 기능"이 포함되어 있는데, 이는 서버가 클라이언트에게 <em>다시</em> 요청을 보낼 수 있음을 의미합니다.[9, 13] 이 양방향 통신 기능은 단순한 요청-응답 관계를 진정한 대화형 상호작용으로 변화시키는, 매우 정교한 설계적 특징입니다.</p>
                    <ul class="list-disc list-outside space-y-2 pl-5">
                        <li><strong>샘플링(Sampling):</strong> 이 기능은 자체 LLM이 없는 경량 프로그램일 수 있는 서버가 하위 작업을 위해 호스트 애플리케이션의 강력한 LLM을 "빌릴" 수 있게 해줍니다. 예를 들어, "코드 분석" 서버는 특정 함수를 찾은 다음, 샘플링을 사용하여 호스트의 LLM에게 "이 함수가 무엇을 하는지 평이한 영어로 설명해주세요"라고 요청할 수 있습니다.[9, 13]</li>
                        <li><strong>유도(Elicitation):</strong> 이 기능은 서버가 실행을 일시 중지하고 호스트의 UI를 통해 사용자에게 추가 정보를 요청할 수 있게 합니다. 예를 들어, `book_flight` 도구는 똑같이 좋은 두 개의 항공편을 찾은 후, 유도를 사용하여 사용자에게 "두 가지 옵션을 찾았습니다. 더 저렴한 오전 항공편을 선호하시나요, 아니면 더 직항인 오후 항공편을 선호하시나요?"라고 물어볼 수 있습니다.[9, 13] 이는 에이전트 워크플로우를 훨씬 더 강력하고 상호작용적으로 만듭니다.</li>
                    </ul>
                </section>

                <hr class="my-12">

                <section id="part2">
                    <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">파트 2: 실전 가이드: MCP와 파이썬으로 Q&A 어시스턴트 만들기</h2>
                    <h3 class="text-2xl font-bold mt-8 mb-4">목표</h3>
                    <p>이 섹션에서는 간단하지만 완전한 검색 증강 생성(Retrieval-Augmented Generation, RAG) 애플리케이션을 구축합니다. MCP 서버는 지식 베이스를 검색하는 단일 도구 `search_knowledge_base`를 노출합니다. MCP 클라이언트 애플리케이션은 LLM을 사용하여 사용자 질문을 받고, 도구를 사용하기로 결정하고, MCP를 통해 실행한 다음, 검색된 정보를 기반으로 최종 답변을 생성합니다. 이 프로젝트는 사용자의 모든 요구사항을 직접적으로 해결합니다.</p>
                    
                    <h3 class="text-2xl font-bold mt-8 mb-4">사전 준비 및 환경 설정</h3>
                    <p>먼저 개발 환경을 설정해야 합니다. 이 과정은 깨끗하고 독립적인 작업 공간을 보장합니다.</p>
                    <ol class="list-decimal list-outside space-y-4 pl-5">
                        <li>
                            <p><strong>Python 및 `uv` 설치:</strong> `uv`는 MCP 파이썬 SDK에서 권장하는 현대적이고 빠른 패키지 관리자입니다.[14, 15] 시스템에 Python 3.8 이상이 설치되어 있는지 확인하고, 다음 명령어를 사용하여 `uv`를 설치합니다.</p>
                            <div class="code-block mt-2">
                                <pre class="bg-gray-800 text-white p-4 rounded-md overflow-x-auto"><code class="language-bash"># macOS / Linux
curl -LsSf https://astral.sh/uv/install.sh | sh
# Windows
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"</code></pre>
                            </div>
                        </li>
                        <li>
                            <p><strong>프로젝트 디렉토리 및 가상 환경 생성:</strong> 프로젝트를 위한 격리된 환경을 만듭니다.[16, 17]</p>
                            <div class="code-block mt-2">
                                <pre class="bg-gray-800 text-white p-4 rounded-md overflow-x-auto"><code class="language-bash">uv init mcp-rag-tutorial
cd mcp-rag-tutorial
uv venv
source.venv/bin/activate  # macOS / Linux
# .venv\Scripts\activate  # Windows</code></pre>
                            </div>
                        </li>
                        <li>
                            <p><strong>필요한 라이브러리 설치:</strong> MCP, OpenAI 호환 라이브러리, Hugging Face 데이터셋 라이브러리를 설치합니다.[14, 18]</p>
                            <div class="code-block mt-2">
                                <pre class="bg-gray-800 text-white p-4 rounded-md overflow-x-auto"><code class="language-bash">uv add "mcp[cli]" openai datasets pandas</code></pre>
                            </div>
                        </li>
                    </ol>

                    <h3 class="text-2xl font-bold mt-8 mb-4">1단계: Hugging Face 데이터셋으로 지식 베이스 준비하기</h3>
                    <p>기업 방화벽에 의해 차단될 수 있는 구글 드라이브와 같은 소스를 피하기 위해, 공개적으로 접근 가능한 Hugging Face의 데이터셋을 사용합니다. 이 작업에는 질의응답(Question-Answering)에 적합한 `squad` 데이터셋이 이상적입니다.[19, 20, 21]</p>
                    <p>먼저, 데이터셋을 로드하고 간단한 인메모리 검색 함수를 준비하여 우리의 "지식 베이스" 역할을 하도록 합니다.</p>
                    
                    <h4 class="text-xl font-semibold mt-6 mb-3">표 1: Hugging Face 데이터셋 프로필</h4>
                    <p>이 표는 애플리케이션이 기반으로 할 데이터에 대한 명확하고 구조화된 요약을 제공합니다. 이를 통해 사용자는 복잡한 MCP 개념에 뛰어들기 전에 지식 베이스의 입력과 출력을 빠르게 이해할 수 있습니다.</p>
                    <div class="overflow-x-auto mt-4">
                        <table class="min-w-full bg-white border border-gray-300">
                            <thead class="bg-gray-100">
                                <tr>
                                    <th class="px-4 py-2 border-b text-left">필드</th>
                                    <th class="px-4 py-2 border-b text-left">설명</th>
                                    <th class="px-4 py-2 border-b text-left">예시 값</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td class="px-4 py-2 border-b"><strong>데이터셋 이름</strong></td>
                                    <td class="px-4 py-2 border-b">Hugging Face의 공식 이름</td>
                                    <td class="px-4 py-2 border-b">`squad` (Stanford Question Answering Dataset)</td>
                                </tr>
                                <tr class="bg-gray-50">
                                    <td class="px-4 py-2 border-b"><strong>주요 특징</strong></td>
                                    <td class="px-4 py-2 border-b">데이터셋의 관련 열</td>
                                    <td class="px-4 py-2 border-b">`question`, `context`, `answers`</td>
                                </tr>
                                <tr>
                                    <td class="px-4 py-2 border-b"><strong>샘플 레코드</strong></td>
                                    <td class="px-4 py-2 border-b">JSON 형식의 전체 예시 레코드</td>
                                    <td class="px-4 py-2 border-b">`{"question": "To whom did the Virgin Mary allegedly appear...", "context": "Architecturally, the school has a Catholic character...", "answers": {"text":,...}}` [21]</td>
                                </tr>
                                <tr class="bg-gray-50">
                                    <td class="px-4 py-2 border-b"><strong>프로젝트 내 사용법</strong></td>
                                    <td class="px-4 py-2 border-b">이 데이터를 사용하는 방법</td>
                                    <td class="px-4 py-2 border-b">사용자의 `question`과 관련된 정보를 찾기 위해 `context`를 검색합니다.</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <p class="mt-4">다음은 데이터셋을 로드하고 검색하는 간단한 파이썬 스크립트(`knowledge_base.py`)입니다.</p>
                    <div class="code-block mt-2">
                        <pre class="bg-gray-800 text-white p-4 rounded-md overflow-x-auto"><code class="language-python"># knowledge_base.py
import pandas as pd
from datasets import load_dataset

# 데이터셋 로드 및 DataFrame으로 변환
dataset = load_dataset("squad", split="train")
df = pd.DataFrame(dataset)

def search_knowledge(query: str) -> str:
    """
    간단한 키워드 검색으로 SQuAD 데이터셋에서 관련 컨텍스트를 찾습니다.
    실제 프로덕션 환경에서는 벡터 검색과 같은 더 정교한 방법이 사용됩니다.
    """
    # 쿼리의 단어가 포함된 컨텍스트 검색
    # 간단한 예제를 위해, 첫 번째 일치 항목을 반환합니다.
    query_words = set(query.lower().split())
    for _, row in df.iterrows():
        if query_words.intersection(set(row['context'].lower().split())):
            return row['context']
    return "관련 정보를 찾을 수 없습니다."

if __name__ == '__main__':
    # 테스트
    print(search_knowledge("Who is the Virgin Mary?"))</code></pre>
                    </div>

                    <h3 class="text-2xl font-bold mt-8 mb-4">2단계: MCP 서버 구축하기 (`qa_server.py`)</h3>
                    <p>이제 우리의 지식 베이스를 LLM이 사용할 수 있는 "도구"로 노출하는 MCP 서버를 만듭니다.</p>
                     <ul class="list-disc list-outside space-y-2 pl-5">
                        <li>공식 파이썬 SDK의 사용하기 쉬운 `FastMCP` 서버 클래스를 사용하여 서버 인스턴스를 생성합니다.[7, 22]</li>
                        <li>앞서 만든 `search_knowledge` 함수를 가져와 `@mcp.tool()` 데코레이터를 사용하여 MCP 도구로 변환합니다. 이 간단한 데코레이터가 마법처럼 우리의 파이썬 함수를 전체 MCP 생태계에 노출시키는 역할을 합니다.[7, 22]</li>
                        <li>함수의 독스트링(docstring)은 단순한 개발자 주석이 아닙니다. 이것은 LLM이 이 도구가 무엇을 하는지, 언제 사용해야 하는지를 이해하는 데 사용하는 자연어 설명이므로 매우 중요합니다.[7]</li>
                    </ul>
                    <div class="code-block mt-2">
                        <pre class="bg-gray-800 text-white p-4 rounded-md overflow-x-auto"><code class="language-python"># qa_server.py
from mcp.server.fastmcp import FastMCP
from knowledge_base import search_knowledge

# MCP 서버 인스턴스 생성
mcp = FastMCP("QAServer")

@mcp.tool()
def search_knowledge_base(query: str) -> str:
    """
    사용자 쿼리와 관련된 정보를 찾기 위해 지식 베이스를 검색합니다.
    일반적인 지식이 아닌 특정 문서나 데이터에 대한 질문에 답할 때 사용하세요.
    """
    return search_knowledge(query)

if __name__ == "__main__":
    # stdio를 통해 서버 실행
    mcp.run(transport="stdio")</code></pre>
                    </div>

                    <h3 class="text-2xl font-bold mt-8 mb-4">3단계: MCP 클라이언트 및 LLM 오케스트레이터 구축하기 (`app.py`)</h3>
                    <p>이 단계는 모든 조각을 하나로 모으는 튜토리얼의 핵심입니다. 사용자의 입력을 받아 LLM과 MCP 서버를 조율하여 답변을 생성하는 클라이언트 애플리케이션을 만듭니다.</p>
                     <ul class="list-disc list-outside space-y-2 pl-5">
                        <li><strong>서버에 연결:</strong> 파이썬 스크립트가 `qa_server.py`를 하위 프로세스로 시작하고 MCP 파이썬 SDK를 사용하여 클라이언트 연결을 설정하도록 합니다.[18, 22]</li>
                        <li><strong>LLM 통합 (OpenAI API 호환성):</strong> 사용자의 요구사항에 따라, `openai` 라이브러리를 사용하되, `base_url`을 사내 로컬 LLM 서비스 엔드포인트로 지정할 수 있도록 구성합니다. 이 예제에서는 OpenAI의 API를 사용하지만, 이 부분은 쉽게 수정 가능합니다.</li>
                        <li><strong>에이전트 루프(Agentic Loop) 구현:</strong>
                            <ol class="list-decimal list-outside space-y-2 pl-5 mt-2">
                                <li>클라이언트는 서버에 연결하고 `session.list_tools()`를 호출하여 `search_knowledge_base` 도구를 발견합니다.[22]</li>
                                <li>LLM에게 도움이 필요한 질문에 답하기 위해 사용 가능한 도구를 활용하는 유용한 어시스턴트라고 지시하는 시스템 프롬프트를 구성합니다.</li>
                                <li>메인 애플리케이션 루프는 사용자 입력을 받습니다 (예: "그로토의 기원은 무엇인가?").</li>
                                <li>사용자 질문과 서버에서 검색한 `search_knowledge_base` 도구의 정의를 OpenAI API 호출의 `tools` 매개변수에 전달합니다.</li>
                                <li>LLM이 도구를 사용하기로 결정하면, 응답에 `tool_calls` 객체가 포함됩니다.</li>
                                <li>클라이언트 코드는 LLM이 제공한 인수로 `session.call_tool()`을 호출하여 도구를 실행합니다.[18, 22]</li>
                                <li>도구의 결과는 새로운 API 호출을 통해 LLM에 다시 전달되어, 사실에 기반한 최종 답변을 생성하게 됩니다. 이 과정은 MCP를 통한 완전한 RAG 워크플로우를 보여줍니다.</li>
                            </ol>
                        </li>
                    </ul>
                    <div class="code-block mt-2">
                        <pre class="bg-gray-800 text-white p-4 rounded-md overflow-x-auto"><code class="language-python"># app.py
import asyncio
import os
from openai import OpenAI
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# 로컬 LLM을 사용하는 경우 base_url을 설정하세요.
# 예: client = OpenAI(base_url="http://localhost:1234/v1", api_key="not-needed")
# 환경 변수에서 OpenAI API 키를 로드합니다.
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# QA 서버를 실행할 명령 정의
server_params = StdioServerParameters(
    command="python",
    args=["qa_server.py"],
)

async def main():
    # qa_server.py를 하위 프로세스로 시작하고 클라이언트 연결
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            # 서버에서 사용 가능한 도구 목록 가져오기
            mcp_tools = await session.list_tools()
            print(f"✅ 서버에서 도구를 성공적으로 로드했습니다: {[tool.name for tool in mcp_tools]}")

            # OpenAI API 형식으로 도구 정의 변환
            openai_tools = [
                {
                    "type": "function",
                    "function": {
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": tool.input_schema,
                    },
                }
                for tool in mcp_tools
            ]

            messages = [
                {"role": "system", "content": "당신은 유용한 AI 어시스턴트입니다. 사용자의 질문에 답하기 위해 필요하다면 제공된 도구를 사용하세요."}
            ]

            while True:
                user_input = input("🤔 질문을 입력하세요 (종료하려면 'q' 입력): ")
                if user_input.lower() == 'q':
                    break

                messages.append({"role": "user", "content": user_input})

                # 1. LLM에게 도구를 사용할지 결정하도록 요청
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    tools=openai_tools,
                    tool_choice="auto",
                )
                response_message = response.choices[0].message

                # 2. LLM이 도구 사용을 결정했는지 확인
                if response_message.tool_calls:
                    messages.append(response_message)
                    
                    for tool_call in response_message.tool_calls:
                        function_name = tool_call.function.name
                        function_args = eval(tool_call.function.arguments) # 간단한 예제를 위해 eval 사용

                        print(f"🤖 LLM이 '{function_name}' 도구를 호출하려고 합니다...")
                        
                        # 3. MCP 클라이언트를 통해 도구 실행
                        tool_result = await session.call_tool(
                            function_name, arguments=function_args
                        )
                        
                        print(f"✅ 도구 실행 완료. 결과 일부: {str(tool_result)[:100]}...")

                        # 4. 도구 실행 결과를 LLM에 다시 전달하여 최종 답변 생성
                        messages.append(
                            {
                                "tool_call_id": tool_call.id,
                                "role": "tool",
                                "name": function_name,
                                "content": str(tool_result),
                            }
                        )
                    
                    final_response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=messages,
                    )
                    print("\n💡 최종 답변:")
                    print(final_response.choices[0].message.content)
                else:
                    # 도구를 사용하지 않고 직접 답변
                    print("\n💡 답변:")
                    print(response_message.content)

                # 다음 대화를 위해 마지막 사용자/어시스턴트 메시지만 유지 (간단한 메모리 관리)
                messages = messages[:1] + messages[-2:]


if __name__ == "__main__":
    asyncio.run(main())</code></pre>
                    </div>

                    <h3 class="text-2xl font-bold mt-8 mb-4">4단계: 전체 애플리케이션 실행 및 테스트</h3>
                    <p>이제 모든 준비가 끝났습니다. 터미널에서 애플리케이션을 실행하고 테스트해 봅시다.</p>
                    <ol class="list-decimal list-outside space-y-4 pl-5">
                        <li>
                            <p>OpenAI API 키를 환경 변수로 설정합니다.</p>
                            <div class="code-block mt-2">
                                <pre class="bg-gray-800 text-white p-4 rounded-md overflow-x-auto"><code class="language-bash">export OPENAI_API_KEY="sk-..."</code></pre>
                            </div>
                        </li>
                        <li>
                            <p>클라이언트 애플리케이션을 실행합니다.</p>
                            <div class="code-block mt-2">
                                <pre class="bg-gray-800 text-white p-4 rounded-md overflow-x-auto"><code class="language-bash">python app.py</code></pre>
                            </div>
                        </li>
                        <li>
                            <p>애플리케이션이 실행되면 질문을 입력합니다. 예를 들어, `squad` 데이터셋에 있는 내용에 대해 질문해 봅시다.</p>
                        </li>
                    </ol>
                    <p class="mt-4"><strong>샘플 상호작용 추적:</strong></p>
                    <div class="code-block mt-2">
                        <pre class="bg-gray-800 text-white p-4 rounded-md overflow-x-auto"><code>✅ 서버에서 도구를 성공적으로 로드했습니다: ['search_knowledge_base']
🤔 질문을 입력하세요 (종료하려면 'q' 입력): To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?
🤖 LLM이 'search_knowledge_base' 도구를 호출하려고 합니다...
✅ 도구 실행 완료. 결과 일부: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golde...
💡 최종 답변:
According to the provided text, the Virgin Mary allegedly appeared to Saint Bernadette Soubirous in 1858 in Lourdes, France.</code></pre>
                    </div>
                    <p class="mt-4">이 추적은 사용자의 질문이 어떻게 LLM에 의해 처리되고, MCP를 통해 도구가 호출되며, 그 결과가 최종적으로 어떻게 사실에 기반한 답변으로 종합되는지를 명확하게 보여줍니다.[23]</p>
                </section>
                
                <hr class="my-12">

                <section id="part3">
                    <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">파트 3: 고급 주제 및 더 넓은 생태계</h2>
                    <h3 class="text-2xl font-bold mt-8 mb-4">전략적 도입: MCP는 언제 올바른 선택인가?</h3>
                    <p>MCP는 강력하지만 모든 프로젝트에 필요한 만병통치약은 아닙니다. "과잉 기술인가, 필수 요소인가?"라는 질문은 매우 현실적인 고려 사항입니다.[3] MCP의 가치는 관리하는 시스템의 복잡성에 비례하여 기하급수적으로 증가합니다.</p>
                    <h4 class="text-xl font-semibold mt-6 mb-3">MCP가 빛을 발하는 시나리오:</h4>
                    <ul class="list-disc list-outside space-y-2 pl-5">
                        <li><strong>다중 테넌트 LLM 서비스:</strong> 여러 고객이나 팀이 일관된 인터페이스로 모델에 접근해야 할 때.</li>
                        <li><strong>서드파티 플러그인 생태계:</strong> ChatGPT 플러그인처럼, 외부 개발자들이 안전하고 표준화된 방식으로 기능을 확장할 수 있는 플랫폼을 구축할 때.</li>
                        <li><strong>엔터프라이즈 플랫폼:</strong> 거버넌스, 보안, 재사용성이 무엇보다 중요한 대규모 기업 환경에서 여러 백엔드 시스템을 통합하는 복잡한 에이전트를 개발할 때 (예: 공급망 최적화 에이전트, 복합 고객 서비스 봇).[3, 11]</li>
                    </ul>
                    <h4 class="text-xl font-semibold mt-6 mb-3">단순함이 최선일 때:</h4>
                    <ul class="list-disc list-outside space-y-2 pl-5">
                        <li>단일 팀이 소유하고 관리하는 작고 독립적인 프로젝트의 경우, MCP의 추상화 계층은 불필요한 오버헤드가 될 수 있습니다. 이럴 때는 직접적인 API 호출이 더 간단하고 효율적일 수 있습니다.[3]</li>
                    </ul>

                    <h3 class="text-2xl font-bold mt-8 mb-4">설계에 의한 보안: MCP의 기본 철학</h3>
                    <p>많은 프로토콜이 보안을 구현 계층(예: TLS 사용)에 전적으로 맡기는 반면, MCP 사양은 다릅니다. "사용자 동의 및 제어" 및 "도구 안전성"과 같은 보안 원칙을 <em>프로토콜</em> 수준에서 명시적으로 정의합니다.[9] 이는 사후 대응이 아닌 사전 예방적인 설계 선택입니다. LLM 에이전트에게 임의의 코드를 실행할 수 있는 능력을 부여하는 것은 본질적으로 위험하다는 점을 인정하고, 프로토콜 철학 자체에 동의 흐름을 내장함으로써, MCP는 강력한 에이전트를 구축하기 위한 훨씬 안전한 기반을 제공합니다.</p>
                    <ul class="list-disc list-outside space-y-2 pl-5">
                        <li><strong>인간 참여(Human-in-the-Loop)의 중요성:</strong> MCP는 사용자가 도구 실행 및 데이터 접근에 <em>반드시</em> 명시적으로 동의해야 한다고 강조합니다. 이는 신뢰할 수 있는 AI 시스템을 구축하기 위해 타협할 수 없는 원칙입니다.[9]</li>
                        <li><strong>개발자를 위한 모범 사례:</strong> 서버 및 클라이언트 개발자 모두를 위한 실행 가능한 보안 지침에는 입력 유효성 검사, 동의를 위한 명확한 UI 제공, 데이터 프라이버시 보호 등이 포함됩니다.[9, 24]</li>
                    </ul>

                    <h3 class="text-2xl font-bold mt-8 mb-4">성장하는 MCP 생태계: 프로토콜 그 이상</h3>
                    <p>MCP는 이론적인 아이디어에 그치지 않고, 강력한 지원을 받는 활발하고 성장하는 프로젝트입니다.</p>
                    <ul class="list-disc list-outside space-y-2 pl-5">
                        <li><strong>공식 SDK:</strong> 파이썬, 타입스크립트, C#, 자바 등 공식적으로 지원되는 SDK 목록과 함께 Microsoft, Google, JetBrains와 같은 주요 기업과의 협력은 업계의 신뢰를 보여주는 신호입니다.[12]</li>
                        <li><strong>서버의 우주:</strong> 개발자들이 즉시 사용하여 개발 시간을 절약할 수 있는 공식 및 커뮤니티 제작 서버(파일 시스템, 데이터베이스, Sentry 등)가 존재합니다.[12, 25]</li>
                        <li><strong>다중 에이전트 시스템으로의 길:</strong> 이러한 상호 운용 가능한 서버 생태계는 정교한 다중 에이전트 시스템을 구축하는 열쇠입니다. 각기 다른 전문 에이전트(각각 다른 MCP 서버를 사용할 수 있음)가 협력하여 복잡한 문제를 해결할 수 있게 됩니다.[11, 25]</li>
                    </ul>
                </section>

                <hr class="my-12">

                <section id="conclusion">
                    <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">결론: AI의 미래는 구성 가능하고 표준화되어 있다</h2>
                    <p>MCP는 AI 개발의 성숙 과정에서 중요한 역할을 합니다. 맞춤형, 장인 정신에 의존하던 통합 방식에서 벗어나 보다 체계적이고 엔지니어링 중심적인 접근 방식으로 나아가는 길을 열어줍니다.</p>
                    <p>LLM이 외부 세계와 상호작용할 수 있는 공통 언어를 제공함으로써, MCP는 차세대 AI 애플리케이션, 즉 견고하고, 확장 가능하며, 진정으로 지능적인 시스템을 가능하게 하는 핵심 기술입니다. 이는 단순한 AI 도구에서 복잡한 AI 생태계로의 전환을 지원하는 아키텍처의 중추가 될 것입니다.</p>
                </section>
            </main>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            // Mobile Menu Toggle
            const mobileMenuButton = document.getElementById('mobile-menu-button');
            const mobileNav = document.getElementById('mobile-nav');
            mobileMenuButton.addEventListener('click', () => {
                mobileNav.classList.toggle('hidden');
            });

            // Close mobile menu on link click
            const mobileNavLinks = mobileNav.querySelectorAll('a');
            mobileNavLinks.forEach(link => {
                link.addEventListener('click', () => {
                    mobileNav.classList.add('hidden');
                });
            });

            // Scrollspy for navigation highlighting
            const sections = document.querySelectorAll('main section');
            const navLinks = document.querySelectorAll('#desktop-nav a, #mobile-nav a');
            
            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        const id = entry.target.getAttribute('id');
                        navLinks.forEach(link => {
                            link.classList.remove('active');
                            if (link.getAttribute('href') === `#${id}`) {
                                link.classList.add('active');
                            }
                        });
                    }
                });
            }, { rootMargin: "-30% 0px -70% 0px" });

            sections.forEach(section => {
                observer.observe(section);
            });

            // Copy to clipboard for code blocks
            const codeBlocks = document.querySelectorAll('.code-block');
            codeBlocks.forEach(block => {
                const code = block.querySelector('code');
                const copyButton = document.createElement('button');
                copyButton.textContent = 'Copy';
                copyButton.className = 'copy-btn';
                
                copyButton.addEventListener('click', () => {
                    navigator.clipboard.writeText(code.textContent).then(() => {
                        copyButton.textContent = 'Copied!';
                        setTimeout(() => {
                            copyButton.textContent = 'Copy';
                        }, 2000);
                    }).catch(err => {
                        console.error('Failed to copy text: ', err);
                    });
                });

                block.appendChild(copyButton);
            });
        });
    </script>

</body>
</html>
